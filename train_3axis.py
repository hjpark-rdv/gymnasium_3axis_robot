import gymnasium as gym
import numpy as np
import pybullet as p
import pybullet_data
from gymnasium import spaces
from stable_baselines3 import PPO
from stable_baselines3.common.env_checker import check_env
from stable_baselines3.common.vec_env import SubprocVecEnv, VecMonitor
import os

from stable_baselines3.common.monitor import Monitor
import matplotlib.pyplot as plt
import numpy as np
from stable_baselines3.common.callbacks import BaseCallback
from datetime import datetime
import csv
import multiprocessing


DBG_GUI_DISPLAY=True
DBG_GRAPH_DISPLAY=True
DBG_PRINT_INFO=False


def make_env():
    return CartesianRobotEnv()

class LivePlotCallback(BaseCallback):
    """
    í•™ìŠµ ì¤‘ ì‹¤ì‹œê°„ ë³´ìƒ ê·¸ë˜í”„ë¥¼ ì—…ë°ì´íŠ¸í•˜ê³  ë³´ìƒì„ CSV íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” ì½œë°± í´ë˜ìŠ¤
    """
    def __init__(self, update_freq=1000, log_dir="./training_logs"):
        super(LivePlotCallback, self).__init__()
        self.update_freq = update_freq
        self.rewards = []
        self.log_dir = log_dir

        # âœ… ë¡œê·¸ ì €ì¥ í´ë” ìƒì„±
        os.makedirs(self.log_dir, exist_ok=True)

        # âœ… í˜„ì¬ ì‹œê°„ ê¸°ë°˜ íŒŒì¼ëª… ìƒì„± (ì˜ˆ: "training_logs/2025_02_23_11_22_33_rewards.csv")
        self.log_file = os.path.join(self.log_dir, f"{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}_rewards.csv")

        # âœ… CSV íŒŒì¼ ìƒì„± (í—¤ë” ì¶”ê°€)
        with open(self.log_file, mode="w", newline="") as file:
            writer = csv.writer(file)
            writer.writerow(["Step", "Mean Reward"])  # ì»¬ëŸ¼ í—¤ë” ì¶”ê°€

        # Matplotlib ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ í™œì„±í™”
        plt.ion()
        self.fig, self.ax = plt.subplots(figsize=(8, 5))
        self.line, = self.ax.plot([], [], label="Episode Reward")

        self.ax.set_xlabel("Training Iterations")
        self.ax.set_ylabel("Mean Reward")
        self.ax.set_title("Live Training Progress")
        self.ax.legend()
        # plt.show()

    def _on_step(self) -> bool:
        # ì¼ì • stepë§ˆë‹¤ ë³´ìƒì„ ê·¸ë˜í”„ì— ì—…ë°ì´íŠ¸ & CSVì— ì €ì¥
        if self.n_calls % self.update_freq == 0:
            # episode_rewards = np.mean(self.training_env.get_attr("last_reward"))  # âœ… ì˜¬ë°”ë¥¸ ë³´ìƒê°’ ê°€ì ¸ì˜¤ê¸°
            episode_rewards = self.locals['rewards'] if 'rewards' in self.locals else 0
            self.rewards.append(episode_rewards)

            
            # âœ… CSVì— ë³´ìƒ ë°ì´í„° ì €ì¥
            with open(self.log_file, mode="a", newline="") as file:
                writer = csv.writer(file)
                writer.writerow([self.n_calls, episode_rewards])

            # ê·¸ë˜í”„ ì—…ë°ì´íŠ¸
            self.line.set_xdata(range(len(self.rewards)))
            self.line.set_ydata(self.rewards)
            self.ax.relim()
            self.ax.autoscale_view()

            if DBG_GRAPH_DISPLAY == True:
                plt.draw()
                plt.pause(0.1)  # ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸

        return True

class CartesianRobotEnv(gym.Env):
    def __init__(self):
        super(CartesianRobotEnv, self).__init__()

        
        self.step_counter = 0
        
        # ê´€ì ˆì˜ ìœ„ì¹˜ í•œê³„ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
        self.joint_limits = {
            'joint_x': (-1.0, 1.0),
            'joint_y': (-1.0, 1.0),
            'joint_z': (-1.0, 1.0)
        }

        # # ê´€ì ˆì˜ ì´ˆê¸° ìœ„ì¹˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
        # self.joint_positions = {
        #     'joint_x': 0.0,
        #     'joint_y': 0.0,
        #     'joint_z': 0.0
        # }
        self.joint_positions = {
            'joint_x': np.random.uniform(-0.5, 0.5),
            'joint_y': np.random.uniform(-0.5, 0.5),
            'joint_z': np.random.uniform(0.2, 0.8)  # zëŠ” ìŒìˆ˜ê°€ ë˜ì§€ ì•Šë„ë¡ ì„¤ì •
        }
        # ê´€ì ˆì˜ ì´ë¦„ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        self.joint_names = list(self.joint_positions.keys())

        # ê´€ì ˆì˜ ê°œìˆ˜ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        self.num_joints = len(self.joint_names)

        # ê´€ì ˆì˜ ìœ„ì¹˜ì™€ ì†ë„ë¥¼ ê´€ì°°ê°’ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        self.observation_space = spaces.Box(
            low=np.array([self.joint_limits[j][0] for j in self.joint_names] + [-np.inf] * self.num_joints),
            high=np.array([self.joint_limits[j][1] for j in self.joint_names] + [np.inf] * self.num_joints),
            dtype=np.float32
        )

        # ê° ê´€ì ˆì— ëŒ€í•œ í˜ì„ ì•¡ì…˜ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
        self.action_space = spaces.Box(
            low=np.array([-1.0] * self.num_joints),
            high=np.array([1.0] * self.num_joints),
            dtype=np.float32
        )

        # PyBullet ë¬¼ë¦¬ ì‹œë®¬ë ˆì´í„°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        if DBG_GUI_DISPLAY == True:
            self.physics_client = p.connect(p.GUI)
        else:
            self.physics_client = p.connect(p.DIRECT)
        
        p.setAdditionalSearchPath(pybullet_data.getDataPath())
        p.setGravity(0, 0, -9.81)

        current_dir = os.path.dirname(os.path.abspath(__file__))
        urdf_path = os.path.join(current_dir, "robot_description/3dof_cartesian_robot.urdf")

        self.robot_id = p.loadURDF(urdf_path, useFixedBase=True)
         # âœ… Joint ì¸ë±ìŠ¤ ì €ì¥
        self.joint_names = ["joint_x", "joint_y", "joint_z"]
        self.joint_indices = {name: i for i, name in enumerate(self.joint_names)}

        # âœ… End-Effectorì˜ ë§í¬ ì¸ë±ìŠ¤ ì°¾ê¸°
        num_joints = p.getNumJoints(self.robot_id)
        self.end_effector_index = None  # End-Effector ì¸ë±ìŠ¤ ì €ì¥ìš© ë³€ìˆ˜

        for i in range(num_joints):
            joint_info = p.getJointInfo(self.robot_id, i)
            link_name = joint_info[12].decode("utf-8")  # ë§í¬ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
            if link_name == "end_effector":
                self.end_effector_index = i  # âœ… End-Effector ì¸ë±ìŠ¤ ì €ì¥
                break
        
        # âœ… end_effectorë¥¼ ì°¾ì§€ ëª»í•œ ê²½ìš° ì˜¤ë¥˜ ì¶œë ¥
        if self.end_effector_index is None:
            raise ValueError("End-Effector ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! URDF íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)

        # ì´ˆê¸° ëª©í‘œ ì¢Œí‘œ ì„¤ì •
        self.target_position = np.random.uniform(low=[0.1, 0.1, 0.1], high=[0.9, 0.9, 0.9])

        # ê¸°ì¡´ ëª©í‘œ ì˜¤ë¸Œì íŠ¸ ì‚­ì œ í›„ ìƒˆë¡œ ìƒì„±
        if hasattr(self, "target_visual_id"):
            p.removeBody(self.target_visual_id)
        
        visual_shape_id = p.createVisualShape(shapeType=p.GEOM_SPHERE, radius=0.05, rgbaColor=[1, 0, 0, 1])
        self.target_visual_id = p.createMultiBody(baseVisualShapeIndex=visual_shape_id, basePosition=self.target_position)

        # ì¡°ì¸íŠ¸ ì´ˆê¸°í™” (í•œê³„ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ì„¤ì •)
        for joint_name in self.joint_names:
            joint_index = self.joint_indices[joint_name]
            initial_position = (self.joint_limits[joint_name][0] + self.joint_limits[joint_name][1]) / 2

            p.resetJointState(self.robot_id, joint_index, targetValue=initial_position)

        return self._get_observation(), {}
    def _update_target_position(self):
        """
        ëª©í‘œ ì¢Œí‘œë¥¼ ë¬´ì‘ìœ„ë¡œ ë³€ê²½í•˜ê³ , ì‹œê°ì  ì˜¤ë¸Œì íŠ¸ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” í•¨ìˆ˜
        """
        self.target_position = np.random.uniform(low=[0.1, 0.1, 0.1], high=[0.9, 0.9, 0.9])

        # âœ… ê¸°ì¡´ ëª©í‘œ ì˜¤ë¸Œì íŠ¸ê°€ ìˆìœ¼ë©´ ìƒˆ ìœ„ì¹˜ë¡œ ì´ë™
        p.resetBasePositionAndOrientation(self.target_visual_id, self.target_position, [0, 0, 0, 1])

        print(f"ğŸ¯ ëª©í‘œ ìœ„ì¹˜ ë³€ê²½ë¨: {self.target_position}")  # ë””ë²„ê¹…ìš© ì¶œë ¥
    def step(self, action):
        
        # âœ… í˜„ì¬ ìŠ¤í… ì¦ê°€
        self.step_counter += 1  

        # âœ… ì¼ì • ê°„ê²©(ì˜ˆ: 500 ìŠ¤í…)ë§ˆë‹¤ ëª©í‘œ ìœ„ì¹˜ ë³€ê²½
        if self.step_counter % 500 == 0:
            self._update_target_position()

        # í˜„ì¬ ê° ì¡°ì¸íŠ¸ì˜ ìƒíƒœ(ìœ„ì¹˜)ë¥¼ ê°€ì ¸ì˜´
        joint_states = p.getJointStates(self.robot_id, list(self.joint_indices.values()))
        current_positions = {name: joint_states[i][0] for i, name in enumerate(self.joint_names)}

        # ë¶€ëª¨ ë§í¬ ìœ„ì¹˜ ë°˜ì˜í•˜ì—¬ ìƒˆë¡œìš´ ëª©í‘œ ìœ„ì¹˜ ê³„ì‚°
        # new_z = max(min(current_positions["joint_z"] + action[2] * 0.05, self.joint_limits["joint_z"][1]), self.joint_limits["joint_z"][0])
        # new_y = max(min(current_positions["joint_y"] + action[1] * 0.05, self.joint_limits["joint_y"][1]), self.joint_limits["joint_y"][0]) + new_z
        # new_x = max(min(current_positions["joint_x"] + action[0] * 0.05, self.joint_limits["joint_x"][1]), self.joint_limits["joint_x"][0]) + new_y

        new_z = max(min(current_positions["joint_z"] + action[2] * 1, self.joint_limits["joint_z"][1]), self.joint_limits["joint_z"][0])
        new_y = max(min(current_positions["joint_y"] + action[1] * 1, self.joint_limits["joint_y"][1]), self.joint_limits["joint_y"][0])
        new_x = max(min(current_positions["joint_x"] + action[0] * 1, self.joint_limits["joint_x"][1]), self.joint_limits["joint_x"][0])


        # ë¶€ëª¨ ìœ„ì¹˜ë¥¼ ë°˜ì˜í•œ ì¡°ì¸íŠ¸ ìœ„ì¹˜ ì„¤ì •
        p.setJointMotorControl2(self.robot_id, self.joint_indices["joint_z"], controlMode=p.POSITION_CONTROL, targetPosition=new_z, force=50)
        p.setJointMotorControl2(self.robot_id, self.joint_indices["joint_y"], controlMode=p.POSITION_CONTROL, targetPosition=new_y, force=50)
        p.setJointMotorControl2(self.robot_id, self.joint_indices["joint_x"], controlMode=p.POSITION_CONTROL, targetPosition=new_x, force=50)

        # PyBullet ì‹œë®¬ë ˆì´ì…˜ í•œ ìŠ¤í… ì§„í–‰
        p.stepSimulation()

        # ë””ë²„ê¹…ìš© ì¶œë ¥ (ê° ì¡°ì¸íŠ¸ì˜ ìœ„ì¹˜ í™•ì¸)
        if DBG_PRINT_INFO == True:
            print(f"ğŸ”„ Step Debug - Z: {new_z:.3f}, Y: {new_y:.3f}, X: {new_x:.3f}")

        # ê´€ì°°ê°’, ë³´ìƒ, ì¢…ë£Œ ì—¬ë¶€ ë°˜í™˜
        observation = self._get_observation()
        reward = self._compute_reward(observation)
        terminated = self._is_terminated(observation)
        truncated = False

        return observation, reward, terminated, truncated, {}

        # # í˜„ì¬ ì¡°ì¸íŠ¸ ìƒíƒœ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
        # for i, joint_name in enumerate(self.joint_names):
        #     joint_index = self.joint_indices[joint_name]
        #     joint_state = p.getJointState(self.robot_id, joint_index)
        #     print(f"Joint {joint_name} - Position: {joint_state[0]:.2f}, Velocity: {joint_state[1]:.2f}")

        # return observation, reward, terminated, truncated, {}


    def _get_observation(self):
        # ê° ê´€ì ˆì˜ ìœ„ì¹˜ì™€ ì†ë„ë¥¼ ê´€ì°°ê°’ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        joint_states = p.getJointStates(self.robot_id, list(self.joint_indices.values()))
        joint_positions = [state[0] for state in joint_states]
        joint_velocities = [state[1] for state in joint_states]
        return np.array(joint_positions + joint_velocities, dtype=np.float32)
    def _get_end_effector_position(self):
        """
        í˜„ì¬ End-Effector(ë¡œë´‡ íŒ” ë) ìœ„ì¹˜ë¥¼ ê°€ì ¸ì˜´.
        """
        # âœ… End-Effectorì˜ ë§í¬ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ„ì¹˜ ê°€ì ¸ì˜¤ê¸°
        link_state = p.getLinkState(self.robot_id, self.end_effector_index)
        end_effector_position = np.array(link_state[0])  # (x, y, z) ì¢Œí‘œ ê°€ì ¸ì˜¤ê¸°
        return end_effector_position
    # def _compute_reward(self, observation):
    #     """
    #     í˜„ì¬ End-Effector ìœ„ì¹˜ì™€ ëª©í‘œ ìœ„ì¹˜ì˜ ê±°ë¦¬ ì°¨ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë³´ìƒì„ ê³„ì‚°.
    #     ëª©í‘œì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ë³´ìƒì„ ì£¼ê³ , ë©€ì–´ì§ˆìˆ˜ë¡ ë³´ìƒì„ ë‚®ì¶˜ë‹¤.
    #     """
    #     end_effector_position = self._get_end_effector_position()
        
    #     # ëª©í‘œ ì¢Œí‘œì™€ í˜„ì¬ ì¢Œí‘œì˜ ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê³„ì‚°
    #     distance_to_target = np.linalg.norm(self.target_position - end_effector_position)

    #     # ë³´ìƒ ê³„ì‚° (ëª©í‘œì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë³´ìƒì´ ì»¤ì§)
    #     reward = -distance_to_target  # ê±°ë¦¬ ìì²´ë¥¼ ë³´ìƒìœ¼ë¡œ ì‚¬ìš© (ì‘ì„ìˆ˜ë¡ ë³´ìƒì´ ë†’ìŒ)
        
    #     return reward
    def _compute_reward(self, observation):
        end_effector_position = self._get_end_effector_position()
        distance_to_target = np.linalg.norm(self.target_position - end_effector_position)

        # âœ… ëª©í‘œì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ ë³´ìƒì„ ì¦ê°€ (ê±°ë¦¬ì˜ ì—­ìˆ˜ë¥¼ ì‚¬ìš©)
        reward = 1.0 / (1.0 + distance_to_target)  

        # âœ… ëª©í‘œ ìœ„ì¹˜ì— ë„ë‹¬í•˜ë©´ ì¶”ê°€ ë³´ìƒ ë¶€ì—¬
        if distance_to_target < 0.05:  
            reward += 5.0  
        print("end_effector_position=",end_effector_position," / self.target_position=",self.target_position,"/ distance",distance_to_target," / reward=",reward)
        return reward


    def _is_terminated(self, observation):
        # ê´€ì ˆì´ í•œê³„ë¥¼ ë²—ì–´ë‚˜ë©´ ì—í”¼ì†Œë“œë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.
        joint_positions = observation[:self.num_joints]
        for i, joint_name in enumerate(self.joint_names):
            if not (self.joint_limits[joint_name][0] <= joint_positions[i] <= self.joint_limits[joint_name][1]):
                return True
        return False

    def render(self, mode='human'):
        pass

    def close(self):
        p.disconnect()

# if __name__ == "__main__":
#     # í™˜ê²½ì„ ìƒì„±í•©ë‹ˆë‹¤.
#     env = CartesianRobotEnv()

#     # âœ… í˜„ì¬ ì‹œê°„ì„ ê¸°ë°˜ìœ¼ë¡œ íŒŒì¼ëª… ìƒì„± (ì˜ˆ: "2025_02_23_11_22_33.csv")
#     current_time = datetime.now().strftime("%Y_%m_%d_%H_%M_%S")
#     log_dir = "./training_logs"
#     os.makedirs(log_dir, exist_ok=True)  # í´ë” ìƒì„±

#     # âœ… CSV íŒŒì¼ ê²½ë¡œ ì„¤ì •
#     log_file = os.path.join(log_dir, f"{current_time}_")

#     # âœ… Monitorì— íŒŒì¼ ê²½ë¡œ ì ìš©
#     env = Monitor(env, log_file)

#     # í™˜ê²½ì´ ì˜¬ë°”ë¥´ê²Œ ì •ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
#     check_env(env, warn=True)

#     # PPO ì—ì´ì „íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
#     model = PPO("MlpPolicy", env, verbose=1)
#     # ì½œë°±ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ì¤‘ ì‹¤ì‹œê°„ ê·¸ë˜í”„ í‘œì‹œ
#     callback = LivePlotCallback(update_freq=1000)
#     # ì—ì´ì „íŠ¸ë¥¼ í•™ìŠµì‹œí‚µë‹ˆë‹¤.
#     model.learn(total_timesteps=500000, callback=callback)

#     # í•™ìŠµëœ ì—ì´ì „íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
#     model.save("ppo_cartesian_robot")

#     # í™˜ê²½ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.
#     env.close()


if __name__ == "__main__":
    # âœ… ì‚¬ìš© ê°€ëŠ¥í•œ CPU ê°œìˆ˜ í™•ì¸ (ìµœëŒ€ 4ê°œ ì‚¬ìš©)
    num_cpu = min(1, multiprocessing.cpu_count())

    # âœ… í™˜ê²½ì„ ë³‘ë ¬ë¡œ ìƒì„±í•˜ëŠ” í•¨ìˆ˜ ì •ì˜
    def make_env():
        return CartesianRobotEnv()

    # âœ… ë©€í‹° í”„ë¡œì„¸ìŠ¤ í™˜ê²½ ìƒì„±
    envs = SubprocVecEnv([lambda: make_env() for _ in range(num_cpu)])

    # âœ… í˜„ì¬ ì‹œê°„ì„ ê¸°ë°˜ìœ¼ë¡œ íŒŒì¼ëª… ìƒì„± (ì˜ˆ: "2025_02_23_11_22_33_")
    current_time = datetime.now().strftime("%Y_%m_%d_%H_%M_%S")
    log_dir = "./training_logs"
    os.makedirs(log_dir, exist_ok=True)  # í´ë” ìƒì„±

    # âœ… CSV íŒŒì¼ ê²½ë¡œ ì„¤ì •
    log_file = os.path.join(log_dir, f"{current_time}_")

    # âœ… VecMonitorë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œê·¸ ì €ì¥ (ë©€í‹° í”„ë¡œì„¸ìŠ¤ ì§€ì›)
    envs = VecMonitor(envs, log_file)

    # âœ… í™˜ê²½ì´ ì˜¬ë°”ë¥´ê²Œ ì •ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸ (ì‹±ê¸€ í™˜ê²½ì—ì„œë§Œ ì‹¤í–‰ ê°€ëŠ¥)
    check_env(make_env(), warn=True)

    # âœ… PPO ëª¨ë¸ ì´ˆê¸°í™” (ë©€í‹°ì½”ì–´ ì§€ì›)
    model = PPO(
        "MlpPolicy",
        envs,
        verbose=1,
        learning_rate=1e-4,  # í•™ìŠµë¥  ì¡°ì •
        ent_coef=0.01,  # íƒìƒ‰ ê°•í™”
        n_steps=2048,  # í•™ìŠµ ì—…ë°ì´íŠ¸ ê°„ê²© ì¦ê°€
        batch_size=64,  # ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°
        n_epochs=10,  # ê° ì—…ë°ì´íŠ¸ ë°˜ë³µ íšŸìˆ˜
    )

    # âœ… ì½œë°±ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ì¤‘ ì‹¤ì‹œê°„ ê·¸ë˜í”„ í‘œì‹œ
    callback = LivePlotCallback(update_freq=1000)

    # âœ… ì—ì´ì „íŠ¸ë¥¼ í•™ìŠµì‹œí‚µë‹ˆë‹¤.
    model.learn(total_timesteps=500000, callback=callback)

    # âœ… í•™ìŠµëœ ì—ì´ì „íŠ¸ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
    model.save("ppo_cartesian_robot")

    # âœ… í™˜ê²½ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.
    envs.close()
